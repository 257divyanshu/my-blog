---
title: LLM Evaluation Technique 2 â€” Embedding Similarity
date: 2025-11-09T12:00:00
description: Learn how embedding similarity helps evaluate AI by comparing meanings, not words â€” a core technique used to measure the quality of LLM responses.
---

Welcome to my blog ğŸ‘‹

When evaluating AI, matching words isnâ€™t enough â€” we need to measure meaning.

In this post, weâ€™ll explore embedding similarity, a technique that turns text into numerical vectors so we can compare how closely two responses align in meaning.
Itâ€™s one of the simplest yet most powerful tools behind modern AI evaluation.

## ğŸ§  Concept Overview

Every word, sentence, or paragraph can be turned into a list of numbers â€” called an **embedding** â€” that represents its *meaning* in multi-dimensional space.
Then, we can **compare** two embeddings to see *how similar* their meanings are.

Think of it like this:

```
"Dog" â†’ [0.7, 0.2, 0.9]
"Cat" â†’ [0.6, 0.25, 0.85]
"Car" â†’ [0.1, 0.8, 0.2]
```

If we plot them in space (imagine a 3D map),
â€œDogâ€ and â€œCatâ€ will be **closer** to each other than to â€œCarâ€.

Thatâ€™s *semantic similarity* â€” â€œdogâ€ and â€œcatâ€ are conceptually closer.

---

### ğŸ§© Diagram

```
         ğŸ¶ Dog â—
                \
                 \
                  â— ğŸ± Cat
                  
         ğŸš— Car                 â— ğŸ Apple
```

â€œDogâ€ and â€œCatâ€ are close â€” high similarity.
â€œDogâ€ and â€œCarâ€ are far apart â€” low similarity.

---

## ğŸ¯ Why Itâ€™s Used

Embedding similarity is **everywhere** in LLM evaluation and retrieval systems.
Itâ€™s used to automatically check *how close an AIâ€™s answer is to a correct or reference answer.*

| Use Case                      | Example                                           |
| ----------------------------- | ------------------------------------------------- |
| **Answer Quality Evaluation** | Compare LLMâ€™s answer to a gold-standard reference |
| **Semantic Search**           | Retrieve documents similar in meaning, not words  |
| **Summarization Evaluation**  | Compare generated summary to reference summary    |
| **Paraphrase Detection**      | Check if two sentences mean the same thing        |

Big companies (OpenAI, Google, Anthropic) use embedding-based metrics like **BERTScore**, **Sentence-BERT Similarity**, or **Cosine Similarity** for evaluating model quality automatically.

---

## âš™ï¸ How It Works

Letâ€™s simplify this into four steps:

---

### ğŸ§© Step 1: Get Embeddings for Each Text

An embedding model (like `text-embedding-004` from Gemini or OpenAIâ€™s `text-embedding-3-small`) is used to convert both:

* Reference text (ground truth or ideal answer)
* Candidate text (modelâ€™s output)

into numerical vectors.

```
Prompt: "What is gravity?"
Reference Answer â†’ Embedding A
Model Answer     â†’ Embedding B
```

---

### ğŸ§© Step 2: Compute Cosine Similarity

The **angle** between these two vectors â€” called *cosine similarity* â€” is calculated.

If two embeddings point in the same direction â†’ similarity is **close to 1**.
If they are opposite â†’ similarity is **close to -1**.

```
cosine_similarity = (A Â· B) / (||A|| * ||B||)
```

(They aren't calculated manually â€” libraries handle it.)

---

### ğŸ§© Step 3: Interpret the Score

| Score | Meaning                                |
| ----- | -------------------------------------- |
| ~1.0  | Almost identical in meaning            |
| ~0.7  | Semantically similar                   |
| ~0.4  | Some overlap, not identical            |
| ~0.0  | Unrelated meaning                      |
| < 0   | Opposite meaning (rare in LLM context) |

---

### ğŸ§© Step 4: Store and Visualize

Similarity scores are stored in the evaluation record:

```json
{
  "prompt": "Explain gravity",
  "model_output": "Gravity pulls objects toward Earth.",
  "reference_answer": "Gravity is the force that pulls objects toward one another.",
  "similarity_score": 0.93
}
```

Now these scores can be visualized in the Analytics Dashboard as a new metric.

---

## ğŸ“Š Example Visualization

```
Prompt: "What is the capital of Japan?"

Reference: "Tokyo"

Model A Output: "Tokyo" â†’ Similarity: 1.00 âœ…
Model B Output: "Osaka" â†’ Similarity: 0.42 âŒ
```

---
## âœï¸ Summary

> When evaluating AI, exact word matching isnâ€™t enough.
> â€œEmbedding similarityâ€ measures how close two texts are in *meaning*, not wording.
> By converting sentences into numerical vectors, we can compute how semantically similar an AIâ€™s response is to a correct one.
> This approach powers tools like BERTScore and is widely used to measure LLM quality.

**The Embedding Similarity technique:**

* Works on *meaning*, not words.
* Uses *cosine similarity* to measure closeness.
* Is easy to compute using embedding APIs.
* Is great for factual, QA, or summarization tasks.

---

Thanks for reading till the end! ğŸ™

Understanding embedding similarity gives you a glimpse into how AI systems truly â€œunderstandâ€ meaning beyond words.

In the next article, weâ€™ll dive deeper into an evaluation technique that evaluates an AI modelâ€™s stability â€” **Self-Consistency Check**.

**See you in the next post ğŸ‘‹**